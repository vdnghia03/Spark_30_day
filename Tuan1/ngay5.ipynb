{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngày 5: Shared Variable và Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Shard Variable\n",
    "Spark theo mặc định không hỗ trợ thay đổi trạng thái biến được chia sẻ trên các Worker Node. Share variables giúp giải quyết một số giới hạn của kiến trúc này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Broadcast Variable**\n",
    "   \n",
    "*Khái niệm*: Broadcast variables dùng để chia sẻ một biến chỉ đọc đến tất cả worker node, nhằm giảm việc truyền dữ liệu lặp đi lặp lại.\n",
    "\n",
    "*Sử dụng:* \n",
    "- Broadcast một từ điển lớn hoặc dữ liệu tham khỏa để tất cả worker node có thể truy cập mà không phải tải lại từ đầu.\n",
    "- Lợi ích của Broadcast variables là giảm băng thông và cải thiện hiệu suất.\n",
    "\n",
    "*Cú pháp và ví dụ:*\n",
    "```python\n",
    "    broadcast_val = sc.broadcast(large_dict)\n",
    "    rdd.map(lambda x : some_function(x , broadcast_var.value))\n",
    "```\n",
    "\n",
    "![BroadCast](image_1/broadcast_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Accumulators:**\n",
    "\n",
    "*Khái niệm:* Accumulators là các biến có thể đếm hoặc tổng hợp dữ liệu trên các worker node, thường dùng để theo dõi số liệu hoặc lỗi mà không ảnh hưởng đến trạng thái ứng dụng.\n",
    "\n",
    "*Sử dụng:* Theo dõi số lượng bản ghi lỗi hoặc số liệu thống kê\n",
    "\n",
    "*Cú pháp và ví dụ:* \n",
    "\n",
    "```python\n",
    "    accumulator = sc.accumulator(0)\n",
    "    \n",
    "    rdd.foreach( lambda x: accumulator.add(1) if x == 'error' else None)\n",
    "```\n",
    "\n",
    "![Accumulator](image_1/accumulator_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ba thành phần chính của Spark UI: **JOB  -  STAGE - TASK**\n",
    "\n",
    "- Một JOB có nhiều STAGE\n",
    "- Một STAGE có nhiều TASK\n",
    "\n",
    "![Spark UI](image_1/SparkUI.png)\n",
    "\n",
    "<p style = \"color:green;\">Vậy JOB, STAGE, TASK được hình thành như thế nào ?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sự hình thành JOB\n",
    "\n",
    "Bản chất 1 JOB trong spark UI là 1 Actions khi ta thực hiện một Application trên RDD.\n",
    "\n",
    "1 JOB = 1 ACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sự hình thành của STAGE\n",
    "\n",
    "- Một JOB bao gồm nhiều Stage, đại diện cho các bước trong quá trình thực thi.\n",
    "- Ở đây ta lưu ý với hiện tượng Shuffle, Wide transformation có suffle còn narrow transformation thì không.\n",
    "- Một Job có nhiều Stage với:\n",
    "  - Số lượng Stage = Số lượng Wide Transformations + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sự hình thành TASK\n",
    "\n",
    "Số lượng **TASK** = Số lượng **PARTITION**\n",
    "\n",
    "_Vậy câu hỏi được đặt ra là, làm thế nào để biết số lượng PARTITION mà SPARK sử lí._\n",
    "\n",
    "Trước tiên phải hiểu về Task và Partition trong kiến trúc Cluster của Spark\n",
    "\n",
    "![executor](image_1/executor.png)\n",
    "\n",
    "Các **task** bên trong các **Executor** là các **Partition**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Vậy SPARK chia partition như thế nào ?_\n",
    "\n",
    "Về kiến thức, việc chia ra các **Executor và Partition** thuộc phạm vi **tối ưu** khi làm việc với Spark\n",
    "\n",
    "- Ở Executor: thường người ta chia ra trong 1 Executor sẽ có 5 Core CPU, mỗi Core CPU sẽ đảm nhiệm 1 task và thực hiện sử lí song song các task đó trong cùng 1 Executor\n",
    "\n",
    "- Ở Partiton(task):\n",
    "    - Mặc định: Spark có cơ chế tự động phân tách Partition và tự động tối ưu số lượng Partion\n",
    "        ```python\n",
    "            from pyspark.sql import SparkSesstion\n",
    "            spark = SparkSession.\\\n",
    "                builder.\\\n",
    "                master(\"local[4]\").\\\n",
    "                appName(\"nb_partition\").\\\n",
    "                getOrCreate()   \n",
    "\n",
    "            orders_schema = \"order_id long, order_date date, customer_id long, order_status string\"\n",
    "\n",
    "            orders_df = spark.read \\\n",
    "                .format(\"csv\") \\\n",
    "                .schema(orders_schema) \\\n",
    "                .load(\"C:/data/orders_1gb.csv\")\n",
    "\n",
    "            spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "            # Kết quả mặc định là 200 -> Tức là Mặc định khi load dữ liệu vào Spark sẽ chia 200 partition\n",
    "\n",
    "            spark.conf.get(\"spark.sql.adaptive.enabled\")\n",
    "            # Tuy nhiên, khi xem spark UI sẽ thấy số lượng Partiton nhỏ hơn rất nhiều\n",
    "            # Nguyên nhân là Spark có cơ chế tối ưu số lượng Partion\n",
    "\n",
    "            spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "            # Tắt tối ưu sẽ thấy số lượng partion là 200 trong Spark UI\n",
    "\n",
    "        ```\n",
    "    - Tuy nhiên, là một Data Engineer, bạn không thể phụ thuộc hoàn toàn vào Spark, tùy dự án cụ thể mà \n",
    "bạn là người data engineer phải tính ra số lượng Partion phù hợp, số executor phù hợp với dự án.\n",
    "\n",
    "    - Theo kinh nghiệm, ta có thể tính số lượng Partition theo công thức sau:\n",
    "        - Nếu 1 File : SL Partition = MAX[ (File_Size)/128MB ,  Số lượng Core]\n",
    "        - Nếu nhiều File: SL Partition = MAX [Số lượng File/(128/file_size_average  +  4MB)  ,  Số lượng Core]\n",
    "\n",
    "![SL Partion](image_1/partition_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài tập Vận Dụng:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 1**: Sử dụng broadcast variables để chia sẻ một từ điển lớn cho tất cả các worker node. Xây dựng một hệ thống dịch thuật sơ khai nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/17 23:22:15 WARN Utils: Your hostname, ubunchuu-Test resolves to a loopback address: 127.0.1.1; using 10.0.230.156 instead (on interface wlo1)\n",
      "24/11/17 23:22:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/17 23:22:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TranslationSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo từ điển dịch thuật lớn\n",
    "\n",
    "translation_dict = {\n",
    "    'hello' :  'xin chào',\n",
    "    'world' : 'thế giới',\n",
    "    'python' : 'ngôn ngữ lập trình Python',\n",
    "    'big data': 'dữ liệu lớn',\n",
    "    'machine learning': 'học máy',\n",
    "    'deep learning': 'học sâu',\n",
    "    'artificial intelligence': 'trí tuệ nhân tạo',\n",
    "    'data science': 'khoa học dữ liệu',\n",
    "    'data engineer': 'kỹ sư dữ liệu',\n",
    "    'is' : 'là',\n",
    "    'and' : 'và',\n",
    "    'or' : 'hoặc',\n",
    "    # Thêm các từ khác\n",
    "}\n",
    "\n",
    "# Ở trên ta chỉ tạo một từ điển đơn giản, thực tế ta có thể tải từ điển dịch từ file hoặc database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast từ điển dịch\n",
    "broadcast_dict = sc.broadcast(translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nguồn dữ liệu\n",
    "text = [\n",
    "    'hello world',\n",
    "    'python is amazing',\n",
    "    'big data and machine learning',\n",
    "    'welcome to technology'\n",
    "]\n",
    "\n",
    "# Thực tế ta có thể đọc dữ liệu từ file hoặc database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo RDD từ nguồn dữ liệu\n",
    "\n",
    "text_rdd = sc.parallelize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm dịch thuật và áp dụng vào MAP\n",
    "\n",
    "def translate_text(text):\n",
    "    # Chia câu thành từng từ\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Dịch từng từ\n",
    "    translated_words = []\n",
    "    for word in words:\n",
    "        # Sử dụng broadcast dictionary\n",
    "        translated_word = broadcast_dict.value.get(word, word)\n",
    "        translated_words.append(translated_word)\n",
    "    \n",
    "    # Ghép lại câu\n",
    "    return ' '.join(translated_words)\n",
    "\n",
    "# Áp dụng hàm dịch thuật vào RDD\n",
    "translated_rdd = text_rdd.map(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bản Dịch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============>                                          (4 + 12) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gốc:    hello world\n",
      "Dịch:   xin chào thế giới\n",
      "---\n",
      "Gốc:    python is amazing\n",
      "Dịch:   ngôn ngữ lập trình Python is amazing\n",
      "---\n",
      "Gốc:    big data and machine learning\n",
      "Dịch:   big data and machine learning\n",
      "---\n",
      "Gốc:    welcome to technology\n",
      "Dịch:   welcome to technology\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In kết quả\n",
    "print(\"Bản Dịch:\")\n",
    "for original, translated in zip(text_rdd.collect(), translated_rdd.collect()):\n",
    "    print(f\"Gốc:    {original}\")\n",
    "    print(f\"Dịch:   {translated}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xem Spark UI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét:\n",
    "- Có 2 JOB thành công\n",
    "- Mỗi JOB có một Stage\n",
    "- Một Stage có 16 task, ở đây dữ liệu quá nhỏ nên số lượng Partition nó lấy chính bằng số lượng Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dừng Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 2:** Kiểm Tra Chất Lượng Dữ Liệu với Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataQualityCheck\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Accumulators\n",
    "total_records = sc.accumulator(0)\n",
    "invalid_email_records = sc.accumulator(0)\n",
    "invalid_age_records = sc.accumulator(0)\n",
    "empty_name_records = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu mẫu\n",
    "raw_data = [\n",
    "    \"John,john@example.com,25\",\n",
    "    \"Alice,,30\",\n",
    "    \"Bob,bob@invalid,22\",\n",
    "    \"Charlie,charlie@example.com,-5\",\n",
    "    \"David,david@example.com,40\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD dữ liệu\n",
    "data_rdd = sc.parallelize(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm kiểm tra chất lượng\n",
    "def validate_record(record):\n",
    "    # Tăng tổng số bản ghi\n",
    "    total_records.add(1)\n",
    "    \n",
    "    # Tách thông tin\n",
    "    parts = record.split(',')\n",
    "    \n",
    "    # Kiểm tra tên rỗng\n",
    "    if not parts[0].strip():\n",
    "        empty_name_records.add(1)\n",
    "        return False\n",
    "    \n",
    "    # Kiểm tra email\n",
    "    if not re.match(r\"[^@]+@[^@]+\\.[^@]+\", parts[1]):\n",
    "        invalid_email_records.add(1)\n",
    "        return False\n",
    "    \n",
    "    # Kiểm tra tuổi\n",
    "    try:\n",
    "        age = int(parts[2])\n",
    "        if age <= 0 or age > 120:\n",
    "            invalid_age_records.add(1)\n",
    "            return False\n",
    "    except ValueError:\n",
    "        invalid_age_records.add(1)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Lọc dữ liệu hợp lệ\n",
    "valid_records = data_rdd.filter(validate_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Báo Cáo Chất Lượng Dữ Liệu:\n",
      "Tổng số bản ghi: 5\n",
      "Bản ghi email không hợp lệ: 2\n",
      "Bản ghi tuổi không hợp lệ: 1\n",
      "Bản ghi tên rỗng: 0\n",
      "Bản ghi hợp lệ: 2\n"
     ]
    }
   ],
   "source": [
    "# Thực thi và in kết quả\n",
    "valid_records.collect()\n",
    "\n",
    "print(\"Báo Cáo Chất Lượng Dữ Liệu:\")\n",
    "print(f\"Tổng số bản ghi: {total_records.value}\")\n",
    "print(f\"Bản ghi email không hợp lệ: {invalid_email_records.value}\")\n",
    "print(f\"Bản ghi tuổi không hợp lệ: {invalid_age_records.value}\")\n",
    "print(f\"Bản ghi tên rỗng: {empty_name_records.value}\")\n",
    "print(f\"Bản ghi hợp lệ: {valid_records.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dừng Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
