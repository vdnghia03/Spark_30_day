{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngày 2: Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Khái niệm RDD\n",
    "\n",
    "*Resilient distributed dataset (RDDs)* là tập dữ liệu phân tán và có khả năng phục hồi\n",
    "\n",
    "a. Phân tán Distributed\n",
    "\n",
    "![Distributed](image_1/distribute.png)\n",
    "\n",
    "**Giải thích**: Hình trên giả sử có một file dữ liệu được lưu ở HDFS. Spark có 3 worker thực hiện song song nhiệm vụ sử lí, các dữ liệu là các block. Sau đó Spark thực hiệnmột transformation load dữ liệu lên **Memory** tạothành 3 partition để xử lí dữ liệu, và việc transformation load dữ liệu lên Memory tạo nên 1 RDD\n",
    "<div style=\"text-align: center; color:green;\">\n",
    "    RDD1 = Load file to Memory\n",
    "</div>\n",
    "- Lưu ý: Các transformation thực chất chỉ để spark ghi nhớ chứ chưa hoàn toàn được thực thi. Đến khi có một\n",
    "action thì spark sẽ thực thi với chiến lược thông minh nhất\n",
    "\n",
    "\n",
    "b. Có khả năng phục hồi (Resilent)\n",
    "\n",
    "![Resilient](image_1/resilient.png)\n",
    "\n",
    "**Giải thích**: Hình trên tạo nên các RDD. Thì giả sử RDD3 bị lỗi, lúc này spark hoàn toàn biết được cha của \n",
    "RDD3 là RDD2, vì vậy nó sẽ từ RDD2 đó tạo lại RDD3 chứ không chạy lại từ đầu.\n",
    "\n",
    "Note: Vì chạy trên memory và chiến lược Lazy Transformation nên spark chạy cực kì nhanh.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Các đặc điểm của RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Immutable (Bất biến)**: Một khi đã tạo ra RDD, bạn không thể thay đổi nó. Thay vào đó, bạn tạo RDD mới từ RDD gốc thông qua các phép biến đổi (transformations).\n",
    "\n",
    "- **Distributed (Phân tán)**: RDD tự động chia nhỏ dữ liệu thành các phân vùng và phân phối chúng qua các nodes trong cụm Spark.\n",
    "\n",
    "- **Fault-tolerant (Chịu lỗi)**: Nếu một phần của RDD bị mất, Spark có thể xây dựng lại nó từ các RDD trước đó nhờ lineage (dòng dõi) của RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tạo RDD:\n",
    "\n",
    "Có hai các tạo: Dùng **parralleize** hoặc **External**s data\n",
    "\n",
    "a. Parallelize: dùng hàm _parallelize()_ để tạo RDD từ danh sách hoặc mảng\n",
    "```python\n",
    "    from pyspark import SparkContext\n",
    "\n",
    "    sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "    # Tạo RDD từ collection\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    rdd = sc.parallelize(data)\n",
    "```\n",
    "\n",
    "b. External data: Tạo từ một tập dữ liệu có sẵn (textFile(), .......)\n",
    "```python\n",
    "    from pyspark import SparkContext\n",
    "    \n",
    "    sc = SparkContext(\"local\", \"RDD Example\")\n",
    "    # Tạo RDD từ tập tin văn bản\n",
    "    file_rdd = sc.textFile(\"hdfs://path/to/file.txt\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Các loại RDD\n",
    "\n",
    "a. <u>__textFile__</u>: Đọc dữ liệu từ file văn bản, mỗi <span style=\"color:#B22222;\">dòng</span> là một phần tử của RDD.\n",
    "```python\n",
    "      text_rdd = sc.textFile(\"hdfs://path/to/textfile.txt\")\n",
    "```\n",
    "\n",
    "\n",
    "b.<u> __wholeTextFiles__:</u> Đọc toàn bộ nội dung file vào RDD dưới dạng <span style=\"color:#B22222;\">(fileName, content). </span> \n",
    "```python\n",
    "      # Đọc toàn bộ nội dung của các file trong thư mục\n",
    "      files_rdd = sc.wholeTextFiles(\"hdfs://path/to/directory\")\n",
    "\n",
    "      # Hiển thị tên file và nội dung của chúng\n",
    "      for file_name, content in files_rdd.take(3):\n",
    "      print(f\"File: {file_name}\")\n",
    "      print(f\"Content: {content}\")\n",
    "```\n",
    "\n",
    "\n",
    "c. <u>__sequenceFile__:</u> Đọc file dạng <span style=\"color:#B22222;\">key-value.</span>\n",
    "```python\n",
    "      # Đọc dữ liệu từ một file Sequence (key-value)\n",
    "      sequence_rdd = sc.sequenceFile(\"hdfs://path/to/sequencefile\"\n",
    "                                    , keyClass=\"org.apache.hadoop.io.Text\"\n",
    "                                    , valueClass=\"org.apache.hadoop.io.IntWritable\")\n",
    "\n",
    "      # Hiển thị key-value của 5 cặp đầu tiên\n",
    "      for key, value in sequence_rdd.take(5):\n",
    "      print(f\"Key: {key}, Value: {value}\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "d. <u>__HadoopRDD__:</u> Sử dụng Hadoop InputFormat để đọc dữ liệu từ HDFS hoặc các nguồn khác.Sử dụng Hadoop InputFormat để đọc dữ liệu từ HDFS hoặc các nguồn khác.\n",
    "```python\n",
    "      # Đọc dữ liệu từ HDFS sử dụng Hadoop InputFormat\n",
    "      hadoop_rdd = sc.hadoopFile(\"hdfs://path/to/input\"\n",
    "                              , inputFormatClass=\"org.apache.hadoop.mapred.TextInputFormat\"\n",
    "                              , keyClass=\"org.apache.hadoop.io.LongWritable\"\n",
    "                              , valueClass=\"org.apache.hadoop.io.Text\")\n",
    "\n",
    "      # Hiển thị key-value của 5 cặp đầu tiên\n",
    "      for key, value in hadoop_rdd.take(5):\n",
    "      print(f\"Key: {key}, Value: {value}\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Các Transformation cơ bản \n",
    "\n",
    "Trong bài này, ta sẽ tìm hiểu về 3 transformation cơ bản là: *Filter*, *Map* và *FlatMap*\n",
    "\n",
    "1. **Filter**: Đúng như tên gọi của nó, dùng để lọc dữ liệu, kết quả là một RDD mới\n",
    "```python\n",
    "    # filter: Lọc các số lẻ\n",
    "    rdd_filtered = rdd.filter(lambda x: x % 2 != 0)\n",
    "```\n",
    "\n",
    "\n",
    "2. **Map**: Áp dụng một hàm lên từng phần tử của RDD, kết quả là một RDD mới.\n",
    "```python\n",
    "    # map: Tính bình phương của từng số\n",
    "    rdd_squared = rdd.map(lambda x: x * x)\n",
    "```\n",
    "\n",
    "3. **FlatMap**\n",
    "```python\n",
    "    # flatMap: Tách câu thành từ\n",
    "    lines = sc.parallelize([\"Hello world\", \"Apache Spark\"])\n",
    "    words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "\n",
    "\n",
    "Thắc mắc: Sự khác biệt giữa Map và flatMap là gì ?\n",
    "<details>\n",
    "  <summary>Đáp án:</summary>\n",
    "    <b>1. map(): </b>\n",
    "    <p>- Mô tả: map() biến đổi từng phần tử của RDD (hoặc tập hợp) bằng cách áp dụng một hàm lên từng phần tử đó. Kết quả là một RDD mới với số lượng phần tử không thay đổi (mỗi phần tử đầu vào tạo ra đúng một phần tử đầu ra). </p>\n",
    "    <p>- Đầu vào/Đầu ra: Một phần tử đầu vào -> Một phần tử đầu ra. </p>\n",
    "    <b>2. flatMap(): </b>\n",
    "        <p>- Mô tả: flatMap() cũng áp dụng một hàm lên từng phần tử của RDD, nhưng khác ở chỗ nó cho phép mỗi phần tử đầu vào ánh xạ thành một hoặc nhiều phần tử đầu ra. Sau đó, các phần tử đầu ra này được \"dẹp phẳng\" (flattened) thành một danh sách duy nhất.. </p>\n",
    "        <p>- Đầu vào/Đầu ra: Một phần tử đầu vào -> Một hoặc nhiều phần tử đầu ra (có thể là 0). </p>\n",
    "\n",
    "    \n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập ngày 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 1: Tạo RDD từ một tập tin văn bản lớn và đếm số lần xuất hiện của từng từ.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/23 13:02:50 WARN Utils: Your hostname, ubunchuu-Test resolves to a loopback address: 127.0.1.1; using 172.18.0.1 instead (on interface br-753e92851dac)\n",
      "24/10/23 13:02:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/23 13:02:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/23 13:03:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount_2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"material_1/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is Nghia',\n",
       " 'I am ten years old',\n",
       " 'I live in Ho Chin Minh City now',\n",
       " 'I study at University of Science, in Ho Chi Minh city, Viet Nam',\n",
       " 'Vinh is my friend. He also live in Ho Chi Minh']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda word :(word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda x, y: x+y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', 1),\n",
       " ('name', 1),\n",
       " ('is', 2),\n",
       " ('am', 1),\n",
       " ('ten', 1),\n",
       " ('years', 1),\n",
       " ('live', 2),\n",
       " ('in', 3),\n",
       " ('Chin', 1),\n",
       " ('now', 1),\n",
       " ('at', 1),\n",
       " ('University', 1),\n",
       " ('of', 1),\n",
       " ('Science,', 1),\n",
       " ('Chi', 2),\n",
       " ('Viet', 1),\n",
       " ('Nam', 1),\n",
       " ('friend.', 1),\n",
       " ('my', 2),\n",
       " ('Nghia', 1),\n",
       " ('I', 3),\n",
       " ('old', 1),\n",
       " ('Ho', 3),\n",
       " ('Minh', 3),\n",
       " ('City', 1),\n",
       " ('study', 1),\n",
       " ('city,', 1),\n",
       " ('Vinh', 1),\n",
       " ('He', 1),\n",
       " ('also', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 2: Lọc ra các dòng dữ liệu thỏa mãn một điều kiện cụ thể từ RDD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ ta lấy dữ liệu từ bài 1 trên, hãy in ra các dòng dữ liệu có chứa string \"Chi Minh\"\n",
    "\n",
    "rdd_filter = rdd1.filter(lambda line: \"Chi Minh\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I study at University of Science, in Ho Chi Minh city, Viet Nam',\n",
       " 'Vinh is my friend. He also live in Ho Chi Minh']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 3: Áp dụng một hàm tùy chỉnh lên từng phần tử của RDD.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta lại lấy dữ liệu từ ví dụ trên, với kết quả rdd2 được làm phảng, mỗi phần tử ta tạo thành một list gồm nó và kí tự spark\n",
    "\n",
    "# Cach 1 : Xu dung lambda\n",
    "\n",
    "rdd_addString = rdd2.map(lambda x: [x,\"Spark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'Spark'],\n",
       " ['my', 'Spark'],\n",
       " ['name', 'Spark'],\n",
       " ['is', 'Spark'],\n",
       " ['Nghia', 'Spark'],\n",
       " ['I', 'Spark'],\n",
       " ['am', 'Spark'],\n",
       " ['ten', 'Spark'],\n",
       " ['years', 'Spark'],\n",
       " ['old', 'Spark'],\n",
       " ['I', 'Spark'],\n",
       " ['live', 'Spark'],\n",
       " ['in', 'Spark'],\n",
       " ['Ho', 'Spark'],\n",
       " ['Chin', 'Spark'],\n",
       " ['Minh', 'Spark'],\n",
       " ['City', 'Spark'],\n",
       " ['now', 'Spark'],\n",
       " ['I', 'Spark'],\n",
       " ['study', 'Spark'],\n",
       " ['at', 'Spark'],\n",
       " ['University', 'Spark'],\n",
       " ['of', 'Spark'],\n",
       " ['Science,', 'Spark'],\n",
       " ['in', 'Spark'],\n",
       " ['Ho', 'Spark'],\n",
       " ['Chi', 'Spark'],\n",
       " ['Minh', 'Spark'],\n",
       " ['city,', 'Spark'],\n",
       " ['Viet', 'Spark'],\n",
       " ['Nam', 'Spark'],\n",
       " ['Vinh', 'Spark'],\n",
       " ['is', 'Spark'],\n",
       " ['my', 'Spark'],\n",
       " ['friend.', 'Spark'],\n",
       " ['He', 'Spark'],\n",
       " ['also', 'Spark'],\n",
       " ['live', 'Spark'],\n",
       " ['in', 'Spark'],\n",
       " ['Ho', 'Spark'],\n",
       " ['Chi', 'Spark'],\n",
       " ['Minh', 'Spark']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_addString.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'Spark'],\n",
       " ['my', 'Spark'],\n",
       " ['name', 'Spark'],\n",
       " ['is', 'Spark'],\n",
       " ['Nghia', 'Spark'],\n",
       " ['I', 'Spark'],\n",
       " ['am', 'Spark'],\n",
       " ['ten', 'Spark'],\n",
       " ['years', 'Spark'],\n",
       " ['old', 'Spark'],\n",
       " ['I', 'Spark'],\n",
       " ['live', 'Spark'],\n",
       " ['in', 'Spark'],\n",
       " ['Ho', 'Spark'],\n",
       " ['Chin', 'Spark'],\n",
       " ['Minh', 'Spark'],\n",
       " ['City', 'Spark'],\n",
       " ['now', 'Spark'],\n",
       " ['I', 'Spark'],\n",
       " ['study', 'Spark'],\n",
       " ['at', 'Spark'],\n",
       " ['University', 'Spark'],\n",
       " ['of', 'Spark'],\n",
       " ['Science,', 'Spark'],\n",
       " ['in', 'Spark'],\n",
       " ['Ho', 'Spark'],\n",
       " ['Chi', 'Spark'],\n",
       " ['Minh', 'Spark'],\n",
       " ['city,', 'Spark'],\n",
       " ['Viet', 'Spark'],\n",
       " ['Nam', 'Spark'],\n",
       " ['Vinh', 'Spark'],\n",
       " ['is', 'Spark'],\n",
       " ['my', 'Spark'],\n",
       " ['friend.', 'Spark'],\n",
       " ['He', 'Spark'],\n",
       " ['also', 'Spark'],\n",
       " ['live', 'Spark'],\n",
       " ['in', 'Spark'],\n",
       " ['Ho', 'Spark'],\n",
       " ['Chi', 'Spark'],\n",
       " ['Minh', 'Spark']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cach 2: Xu dung ham do minh viet\n",
    "\n",
    "def addString_2(x):\n",
    "    return [x,\"Spark\"]\n",
    "\n",
    "rdd_add_2 = rdd2.map(addString_2)\n",
    "\n",
    "rdd_add_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
