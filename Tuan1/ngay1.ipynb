{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngày 1: Giới thiệu về Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Big Data** là gì ?\n",
    "**Big Data** là thuật ngữ dùng để mô tả khối lượng dữ liệu rất lớn và phức tạp, thường được tạo ra với tốc độ nhanh từ nhiều nguồn khác nhau. Do kích thước và độ phức tạp của nó, Big Data không thể được quản lý, xử lý hoặc phân tích bằng các công cụ và kỹ thuật truyền thống. Thay vào đó, cần sử dụng các công nghệ và công cụ đặc biệt để xử lý và khai thác giá trị từ nó.\n",
    "\n",
    "Đặc điểm của Big Data (3V + 2V):\n",
    "- Volume(Khối lượng)\n",
    "- Velocity(Tốc độ)\n",
    "- Variety(Đa dạng)\n",
    "- Veracity(Độ chính xác)\n",
    "- Value(Giá trị)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spark là gì, ưu điểm của Spark\n",
    "\n",
    "- Spark: Là một tool xử lí và phân tích dữ liệu lớn\n",
    "- Ưu điểm: \n",
    "    - Advanced Analytics: Spark không chỉ hỗ trợ \"Map\" và \"Reduce \", nó còn hỗ trợ Spark truy vấn SQL, Streaming data, Machine learning (ML) và các thuật toán xử lý đồ thị đóng vai trò như một bộ công cụ phân tích dữ liệu cực kì mạnh mẽ.\n",
    "    - Speed: Spark giúp chạy một ứng dụng với tốc độ rất nhanh. So với Hadoop cluster, Spark Application nến chạy trên bộ nhớ nhanh hơn tới 100 lần và nhanh hơn 10 lần khi chạy trên đĩa. Điều này có được nhờ giảm số lượng các hoạt động đọc / ghi vào ổ đĩa.\n",
    "    - Supports multiple languages: Spark cung cấp built-in APIs phổ biến từ Java, Scala đến Python, R. Do đó, có thể code Spark applications với nhiều lựa chọn về ngôn ngữ lập trình. Bên cạnh đó Spark còn cung cấp rất nhiều high-level operators cho việc truy vấn dữ liệu..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Kiến trúc tổng quan\n",
    "\n",
    "- Tổng quát\n",
    "![architech](image_1/spark_architech.png)\n",
    "\n",
    "- Chi tiết\n",
    "![architech](image_1/architech_complex.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Các thành phần"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Spark** có **2 layer**:\n",
    "- Layer 1: Spark Core API (Chủ yếu sử lí RDD)\n",
    "- Layer 2: High level API\n",
    "\n",
    "![Component](image_1/layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. So sánh với Hadoop MapReduce\n",
    "\n",
    "Spark được sinh ra để cải thiện hiệu xuất của MapReduce của Hadoop\n",
    "\n",
    "Hiệu suất: Spark nhanh hơn map reduce từ 10 đến 100 lần\n",
    "\n",
    "\n",
    "- Mapreduce\\\n",
    "![mapreduce](image_1/mapreduce.png)\n",
    "\n",
    "- Spark\\\n",
    "![spark](image_1/spark.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tâp ngày 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 1**:  Cài đặt Spark trên máy cục bộ và chạy ứng dụng \"Word Count\" mẫu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/21 23:21:32 WARN Utils: Your hostname, ubunchuu-Test resolves to a loopback address: 127.0.1.1; using 10.0.232.111 instead (on interface wlo1)\n",
      "24/10/21 23:21:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/21 23:21:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/21 23:21:45 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"material_1/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello my name is Nghia',\n",
       " 'I am ten years old',\n",
       " 'I live in Ho Chin Minh City now',\n",
       " 'I study at University of Science, in Ho Chi Minh city, Viet Nam',\n",
       " 'Vinh is my friend. He also live in Ho Chi Minh']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Nghia',\n",
       " 'I',\n",
       " 'am',\n",
       " 'ten',\n",
       " 'years',\n",
       " 'old',\n",
       " 'I',\n",
       " 'live',\n",
       " 'in',\n",
       " 'Ho',\n",
       " 'Chin',\n",
       " 'Minh',\n",
       " 'City',\n",
       " 'now',\n",
       " 'I',\n",
       " 'study',\n",
       " 'at',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Science,',\n",
       " 'in',\n",
       " 'Ho',\n",
       " 'Chi',\n",
       " 'Minh',\n",
       " 'city,',\n",
       " 'Viet',\n",
       " 'Nam',\n",
       " 'Vinh',\n",
       " 'is',\n",
       " 'my',\n",
       " 'friend.',\n",
       " 'He',\n",
       " 'also',\n",
       " 'live',\n",
       " 'in',\n",
       " 'Ho',\n",
       " 'Chi',\n",
       " 'Minh']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda word :(word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 1),\n",
       " ('my', 1),\n",
       " ('name', 1),\n",
       " ('is', 1),\n",
       " ('Nghia', 1),\n",
       " ('I', 1),\n",
       " ('am', 1),\n",
       " ('ten', 1),\n",
       " ('years', 1),\n",
       " ('old', 1),\n",
       " ('I', 1),\n",
       " ('live', 1),\n",
       " ('in', 1),\n",
       " ('Ho', 1),\n",
       " ('Chin', 1),\n",
       " ('Minh', 1),\n",
       " ('City', 1),\n",
       " ('now', 1),\n",
       " ('I', 1),\n",
       " ('study', 1),\n",
       " ('at', 1),\n",
       " ('University', 1),\n",
       " ('of', 1),\n",
       " ('Science,', 1),\n",
       " ('in', 1),\n",
       " ('Ho', 1),\n",
       " ('Chi', 1),\n",
       " ('Minh', 1),\n",
       " ('city,', 1),\n",
       " ('Viet', 1),\n",
       " ('Nam', 1),\n",
       " ('Vinh', 1),\n",
       " ('is', 1),\n",
       " ('my', 1),\n",
       " ('friend.', 1),\n",
       " ('He', 1),\n",
       " ('also', 1),\n",
       " ('live', 1),\n",
       " ('in', 1),\n",
       " ('Ho', 1),\n",
       " ('Chi', 1),\n",
       " ('Minh', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda x, y: x+y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', 1),\n",
       " ('name', 1),\n",
       " ('is', 2),\n",
       " ('am', 1),\n",
       " ('ten', 1),\n",
       " ('years', 1),\n",
       " ('live', 2),\n",
       " ('in', 3),\n",
       " ('Chin', 1),\n",
       " ('now', 1),\n",
       " ('at', 1),\n",
       " ('University', 1),\n",
       " ('of', 1),\n",
       " ('Science,', 1),\n",
       " ('Chi', 2),\n",
       " ('Viet', 1),\n",
       " ('Nam', 1),\n",
       " ('friend.', 1),\n",
       " ('my', 2),\n",
       " ('Nghia', 1),\n",
       " ('I', 3),\n",
       " ('old', 1),\n",
       " ('Ho', 3),\n",
       " ('Minh', 3),\n",
       " ('City', 1),\n",
       " ('study', 1),\n",
       " ('city,', 1),\n",
       " ('Vinh', 1),\n",
       " ('He', 1),\n",
       " ('also', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 2: Nghiên cứu các trường hợp sử dụng Spark trong các ngành công nghiệp khác nhau.**\n",
    "\n",
    "**Spark** là một hệ thống xử lý phân tán đa mục đích được sử dụng cho khối lượng công việc có dữ liệu lớn. Hệ thống này đã được triển khai trong mọi loại trường hợp sử dụng dữ liệu lớn để phát hiện các mẫu và cung cấp thông tin chuyên sâu theo thời gian thực. Các ví dụ về trường hợp sử dụng bao gồm:\n",
    "\n",
    "<p style = \"color:green;\">Dịch vụ tài chính:</p>\n",
    "    - Spark được sử dụng trong ngân hàng để dự đoán tỷ lệ khách hàng rời bỏ và đề xuất các sản phẩm tài chính mới. Trong ngân hàng đầu tư, Spark được sử dụng để phân tích giá cổ phiếu nhằm dự đoán xu hướng trong tương lai.\n",
    "<p style = \"color:green;\">Chăm sóc sức khỏe:</p>\n",
    "    - Spark được sử dụng để xây dựng dịch vụ chăm sóc bệnh nhân toàn diện bằng cách cung cấp dữ liệu cho nhân viên y tế tuyến đầu để phục vụ cho mọi tương tác với bệnh nhân. Spark cũng có thể được sử dụng để dự đoán/đề xuất phương pháp điều trị cho bệnh nhân.\n",
    "<p style = \"color:green;\">Sản suất:</p>\n",
    "    - Spark được sử dụng để loại bỏ thời gian ngừng hoạt động của thiết bị kết nối internet bằng cách đề xuất thời điểm thực hiện bảo trì phòng ngừa.\n",
    "<p style = \"color:green;\">Bán lẻ:</p>\n",
    "    - Spark được sử dụng để thu hút và giữ chân khách hàng thông qua các dịch vụ và ưu đãi được cá nhân hóa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 3: So sánh và đối chiếu Spark với các công cụ Big Data khác như Flink và Storm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Tiêu chí**              | **Apache Spark**                                                        | **Apache Flink**                                                     |\n",
    "|---------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Mô hình xử lý**          | Xử lý theo batch và stream (dòng), chủ yếu tập trung vào batch.         | Chuyên về xử lý streaming (xử lý dòng liên tục) với hỗ trợ batch.    |\n",
    "| **Latentcy (Độ trễ)**      | Độ trễ cao hơn do tập trung vào xử lý batch.                            | Độ trễ thấp hơn, tối ưu cho xử lý thời gian thực (real-time).        |\n",
    "| **Fault Tolerance (Khả năng phục hồi)** | Sử dụng DAG lineage để khôi phục dữ liệu khi xảy ra lỗi.               | Sử dụng snapshot và checkpointing để phục hồi dữ liệu một cách nhanh chóng. |\n",
    "| **API lập trình**          | Hỗ trợ nhiều ngôn ngữ: Scala, Java, Python, R.                          | Tập trung vào Java và Scala, hỗ trợ Python qua API PyFlink.          |\n",
    "| **Hiệu suất**              | Hiệu quả cao hơn trong xử lý batch.                                      | Hiệu quả hơn trong xử lý stream nhờ cơ chế event-time.               |\n",
    "| **Kiến trúc xử lý dữ liệu** | Sử dụng micro-batch để xử lý dữ liệu theo dòng.                         | Native stream processing với event-time, hỗ trợ xử lý nhiều sự kiện. |\n",
    "| **Cộng đồng và ứng dụng**   | Rất phổ biến, nhiều ứng dụng trong các công ty lớn và ngành công nghiệp. | Đang phát triển, phổ biến trong các ứng dụng streaming.              |\n",
    "| **Tích hợp hệ sinh thái**  | Tích hợp tốt với Hadoop, HDFS, Hive, và nhiều công cụ trong hệ sinh thái Big Data. | Tích hợp tốt với hệ thống stream processing và lưu trữ thời gian thực. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Tiêu chí**                          | **Apache Spark**                                                                            | **Apache Flink**                                                                         |\n",
    "|---------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| **Mô hình xử lý**                     | Chủ yếu xử lý batch, hỗ trợ stream qua micro-batch.                                          | Native stream processing, hỗ trợ batch, tối ưu cho xử lý real-time.                      |\n",
    "| **API và thư viện**                   | Hỗ trợ nhiều ngôn ngữ lập trình (Scala, Java, Python, R) và thư viện như MLlib, GraphX.      | API chủ yếu cho Java, Scala, hỗ trợ Python qua PyFlink. Tích hợp mạnh với thư viện streaming. |\n",
    "| **Tính chịu lỗi**                     | Sử dụng DAG lineage và checkpointing trong Spark Streaming.                                  | Sử dụng snapshot và checkpointing với event-time để đảm bảo tính chính xác.              |\n",
    "| **Windowing**                         | Hỗ trợ windowing nhưng chủ yếu dựa trên processing-time, hạn chế trong stream processing.    | Hỗ trợ mạnh mẽ windowing dựa trên event-time và processing-time, linh hoạt hơn.          |\n",
    "| **Tốc độ xử lý**                      | Xử lý batch nhanh, độ trễ trong xử lý stream cao hơn do mô hình micro-batch.                | Xử lý stream tốc độ cao với độ trễ thấp, hỗ trợ tốt real-time analytics.                 |\n",
    "| **Tính toán bộ nhớ**                  | Cơ chế quản lý bộ nhớ dựa trên JVM, hỗ trợ spill-to-disk khi bộ nhớ không đủ.               | Quản lý bộ nhớ tốt hơn trong stream processing nhờ mô hình memory-managed.               |\n",
    "| **Quản lý tài nguyên**                | Tích hợp tốt với YARN, Kubernetes, Mesos để quản lý tài nguyên.                             | Hỗ trợ YARN, Mesos, Kubernetes. Cơ chế quản lý tài nguyên tối ưu cho xử lý stream.       |\n",
    "| **Thực thi truy vấn và linh hoạt**    | Tối ưu cho xử lý batch, hỗ trợ stream nhưng linh hoạt thấp hơn trong real-time.             | Linh hoạt trong xử lý stream real-time, tối ưu cho event-driven processing.              |\n",
    "| **Xử lý backpressure**                | Hỗ trợ xử lý backpressure nhưng không mạnh mẽ như Flink trong real-time stream.             | Xử lý backpressure tốt hơn, giúp đảm bảo xử lý hiệu quả với khối lượng dữ liệu lớn.      |\n",
    "| **Phân vùng dữ liệu**                 | Phân vùng dữ liệu chủ yếu cho xử lý batch, hạn chế trong việc phân vùng stream.             | Hỗ trợ phân vùng dữ liệu linh hoạt hơn, tối ưu cho stream processing.                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
